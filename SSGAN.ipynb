{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSGAN.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "OiJzfP_d88xN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "                ############ Imports ############\n",
        "                \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AxbIEmw49wuP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "                ############ Initializations ############            \n",
        "\n",
        "num_classes = 10\n",
        "channels = 1\n",
        "height = 64\n",
        "width = 64\n",
        "# MNIST was resized to 64 * 64 for discriminator and generator architecture fitting\n",
        "latent = 100\n",
        "epsilon = 1e-7\n",
        "labeled_rate = 0.2 # For initial testing\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WVepUf_2khFX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "                ############ Importing MNIST data ############                \n",
        "\n",
        "def get_data():\n",
        "    from tensorflow.examples.tutorials.mnist import input_data\n",
        "    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True, reshape=[])\n",
        "    return mnist\n",
        "\n",
        "                ############ Normalizing data ############\n",
        "# Scaling in range (-1,1) for generator tanh output\n",
        "def scale(x):\n",
        "    # normalize data\n",
        "    x = (x - 0.5) / 0.5\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E7ldhkfXbfjW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Discriminator and Generator architecture should mirror each other"
      ]
    },
    {
      "metadata": {
        "id": "hX3LbUcp_rDJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "              ############ Defining Discriminator ############\n",
        "  \n",
        "def discriminator(x, dropout_rate = 0., is_training = True, reuse = False):\n",
        "    # input x -> n+1 classes\n",
        "    \n",
        "    with tf.variable_scope('Discriminator', reuse = reuse): \n",
        "      \n",
        "      # x = ?*64*64*1\n",
        "      \n",
        "      print('Discriminator architecture: ')\n",
        "      #Layer 1\n",
        "      conv1 = tf.layers.conv2d(x, 128, kernel_size = [4,4], strides = [2,2],\n",
        "                              padding = 'same', activation = tf.nn.leaky_relu, name = 'conv1') # ?*32*32*128\n",
        "      print(conv1.shape)\n",
        "      #No batch-norm for input layer\n",
        "      dropout1 = tf.nn.dropout(conv1, dropout_rate)\n",
        "      \n",
        "      #Layer2\n",
        "      conv2 = tf.layers.conv2d(dropout1, 256, kernel_size = [4,4], strides = [2,2],\n",
        "                              padding = 'same', activation = tf.nn.leaky_relu, name = 'conv2') # ?*16*16*256\n",
        "      batch2 = tf.layers.batch_normalization(conv2, training = is_training)\n",
        "      dropout2 = tf.nn.dropout(batch2, dropout_rate)\n",
        "      print(conv2.shape)\n",
        "      \n",
        "      #Layer3\n",
        "      conv3 = tf.layers.conv2d(dropout2, 512, kernel_size = [4,4], strides = [4,4],\n",
        "                              padding = 'same', activation = tf.nn.leaky_relu, name = 'conv3') # ?*4*4*512\n",
        "      batch3 = tf.layers.batch_normalization(conv3, training = is_training)\n",
        "      dropout3 = tf.nn.dropout(batch3, dropout_rate)\n",
        "      print(conv3.shape)\n",
        "        \n",
        "      # Layer 4\n",
        "      conv4 = tf.layers.conv2d(dropout3, 1024, kernel_size=[3,3], strides=[1,1],\n",
        "                               padding='valid',activation = tf.nn.leaky_relu, name='conv4') # ?*2*2*1024\n",
        "      # No batch-norm as this layer's op will be used in feature matching loss\n",
        "      # No dropout as feature matching needs to be definite on logits\n",
        "      print(conv4.shape)\n",
        "      \n",
        "      # Layer 5\n",
        "      # Note: Applying Global average pooling\n",
        "        \n",
        "      flatten = tf.reduce_mean(conv4, axis = [1,2])\n",
        "      logits_D = tf.layers.dense(flatten, (1 + num_classes))\n",
        "      out_D = tf.nn.softmax(logits_D)\n",
        "        \n",
        "    return flatten,logits_D,out_D\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "635RFlWgFk8A",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "                ############ Defining Generator ############\n",
        "                \n",
        "def generator(z, dropout_rate = 0., is_training = True, reuse = False):\n",
        "    # input latent z -> image x\n",
        "    \n",
        "    with tf.variable_scope('Generator', reuse = reuse):\n",
        "      print('\\n Generator architecture: ')\n",
        "      \n",
        "      #Layer 1\n",
        "      deconv1 = tf.layers.conv2d_transpose(z, 512, kernel_size = [4,4],\n",
        "                                         strides = [1,1], padding = 'valid',\n",
        "                                        activation = tf.nn.relu, name = 'deconv1') # ?*4*4*512\n",
        "      batch1 = tf.layers.batch_normalization(deconv1, training = is_training)\n",
        "      dropout1 = tf.nn.dropout(batch1, dropout_rate)\n",
        "      print(deconv1.shape)\n",
        "      \n",
        "      #Layer 2\n",
        "      deconv2 = tf.layers.conv2d_transpose(dropout1, 256, kernel_size = [4,4],\n",
        "                                         strides = [4,4], padding = 'same',\n",
        "                                        activation = tf.nn.relu, name = 'deconv2')# ?*16*16*256\n",
        "      batch2 = tf.layers.batch_normalization(deconv2, training = is_training)\n",
        "      dropout2 = tf.nn.dropout(batch2, dropout_rate)\n",
        "      print(deconv2.shape)\n",
        "        \n",
        "      #Layer 3\n",
        "      deconv3 = tf.layers.conv2d_transpose(dropout2, 128, kernel_size = [4,4],\n",
        "                                         strides = [2,2], padding = 'same',\n",
        "                                        activation = tf.nn.relu, name = 'deconv3')# ?*32*32*256\n",
        "      batch3 = tf.layers.batch_normalization(deconv3, training = is_training)\n",
        "      dropout3 = tf.nn.dropout(batch3, dropout_rate)\n",
        "      print(deconv3.shape)\n",
        "      \n",
        "      #Output layer\n",
        "      deconv4 = tf.layers.conv2d_transpose(dropout3, 1, kernel_size = [4,4],\n",
        "                                        strides = [2,2], padding = 'same',\n",
        "                                        activation = None, name = 'deconv4')# ?*64*64*1\n",
        "      out = tf.nn.tanh(deconv4)\n",
        "      print(deconv4.shape)\n",
        "    \n",
        "    return out  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uNYagCnMJaLS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "                ############ Building model ############\n",
        "                \n",
        "def build_GAN(x_real, z, dropout_rate, is_training):\n",
        "    \n",
        "    fake_images = generator(z, dropout_rate, is_training)\n",
        "    \n",
        "    D_real_features, D_real_logits, D_real_prob = discriminator(x_real, dropout_rate, \n",
        "                                                              is_training)\n",
        "    \n",
        "    D_fake_features, D_fake_logits, D_fake_prob = discriminator(fake_images, dropout_rate,\n",
        "                                                                is_training, reuse = True)\n",
        "    #Setting reuse=True this time for using variables trained in real batch training \n",
        "    \n",
        "    return D_real_features, D_real_logits, D_real_prob, D_fake_features, D_fake_logits, D_fake_prob, fake_images \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e4JqKKmsawd1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "                ############ Preparing Mask ############\n",
        "  \n",
        "# Preparing a binary label_mask to be multiplied with real labels\n",
        "def get_labeled_mask(labeled_rate, batch_size):\n",
        "    labeled_mask = np.zeros([batch_size], dtype = np.float32)\n",
        "    labeled_count = np.int(batch_size * labeled_rate)\n",
        "    labeled_mask[range(labeled_count)] = 1.0\n",
        "    np.random.shuffle(labeled_mask)\n",
        "    return labeled_mask\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sYDk-3Cd0q9j",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "               ############ Preparing Extended label ############\n",
        "  \n",
        "def prepare_extended_label(label):\n",
        "    # add extra label for fake data\n",
        "    extended_label = tf.concat([tf.zeros([tf.shape(label)[0], 1]), label], axis = 1)\n",
        "\n",
        "    return extended_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iGyS5eI1JeE7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "            ############ Defining losses ############\n",
        "            \n",
        "# The total loss inculcates  D_L_Unsupervised + D_L_Supervised + G_feature_matching loss + G_R/F loss    \n",
        "\n",
        "def loss_accuracy(D_real_features, D_real_logit, D_real_prob, D_fake_features,\n",
        "                  D_fake_logit, D_fake_prob, extended_label, labeled_mask):\n",
        "    \n",
        "                    ### Discriminator loss ###\n",
        "    \n",
        "    # Supervised loss -> which class the real data belongs to\n",
        "    \n",
        "    temp = tf.nn.softmax_cross_entropy_with_logits_v2(logits = D_real_logit,\n",
        "                                                  labels = extended_label) \n",
        "    # Don't confuse labeled_rate with labeled_mask\n",
        "    # Labeled_mask and temp are of same size = batch_size where temp is softmax\n",
        "    # cross_entropy calculated over whole batch\n",
        "    \n",
        "    D_L_Supervised = tf.reduce_sum(tf.multiply(temp,labeled_mask)) / tf.reduce_sum(labeled_mask)\n",
        "    \n",
        "    # Multiplying temp with labeled_mask gives supervised loss on labeled_mask\n",
        "    # data only, calculating mean by dividing by no of labeled samples\n",
        "    \n",
        "    # Unsupervised loss -> R/F\n",
        "    \n",
        "    D_L_RealUnsupervised = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits = D_real_logit[:, 0], labels = tf.zeros_like(D_real_logit[:, 0], dtype=tf.float32)))\n",
        "    \n",
        "    D_L_FakeUnsupervised = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits = D_fake_logit[:, 0], labels = tf.ones_like(D_fake_logit[:, 0], dtype=tf.float32)))\n",
        "    \n",
        "    D_L = D_L_Supervised + D_L_RealUnsupervised + D_L_FakeUnsupervised\n",
        "    \n",
        "    \n",
        "                    ### Generator loss ###\n",
        "                    \n",
        "    # G_L_1 -> Fake data wanna be real \n",
        "    \n",
        "    G_L_1 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits = D_fake_logit[:, 0],labels = tf.zeros_like(D_fake_logit[:, 0], dtype=tf.float32)))\n",
        "    \n",
        "    # G_L_2 -> Feature matching\n",
        "    data_moments = tf.reduce_mean(D_real_features, axis = 0)\n",
        "    sample_moments = tf.reduce_mean(D_fake_features, axis = 0)\n",
        "    G_L_2 = tf.reduce_mean(tf.square(data_moments-sample_moments))\n",
        "    \n",
        "    \n",
        "    G_L = G_L_1 + G_L_2\n",
        "    \n",
        "    prediction = tf.equal(tf.argmax(D_real_prob[:, 1:], 1),\n",
        "                                  tf.argmax(extended_label[:, 1:], 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
        "    \n",
        "    return D_L, G_L, accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-v8g1_2ZJhkF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "            ############ Defining Optimizer ############\n",
        "  \n",
        "def optimizer(D_Loss, G_Loss, learning_rate, beta1):\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        all_vars = tf.trainable_variables()\n",
        "        D_vars = [var for var in all_vars if var.name.startswith('Discriminator')]\n",
        "        G_vars = [var for var in all_vars if var.name.startswith('Generator')]\n",
        "        \n",
        "        d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1,\n",
        "                                             name = 'd_optimiser').minimize(D_Loss, var_list=D_vars)\n",
        "        g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1,\n",
        "                                             name = 'g_optimiser').minimize(G_Loss, var_list=G_vars)\n",
        "            \n",
        "    return d_train_opt, g_train_opt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WbrCsvtrM4VF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "            ############ Plotting Results ############\n",
        "  \n",
        "def show_result(test_images, num_epoch, show = True, save = False, path = 'result.png'):\n",
        "\n",
        "    size_figure_grid = 5\n",
        "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n",
        "    for i in range(0, size_figure_grid):\n",
        "      for j in range(0, size_figure_grid):\n",
        "        ax[i, j].get_xaxis().set_visible(False)\n",
        "        ax[i, j].get_yaxis().set_visible(False)\n",
        "\n",
        "    for k in range(size_figure_grid*size_figure_grid):\n",
        "        i = k // size_figure_grid\n",
        "        j = k % size_figure_grid\n",
        "        ax[i, j].cla()\n",
        "        ax[i, j].imshow(np.reshape(test_images[k], (64, 64)), cmap='gray')\n",
        "\n",
        "    label = 'Epoch {0}'.format(num_epoch)\n",
        "    fig.text(0.5, 0.04, label, ha='center')\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
        "    \n",
        "    \n",
        "    x = range(len(hist['D_losses']))\n",
        "\n",
        "    y1 = hist['D_losses']\n",
        "    y2 = hist['G_losses']\n",
        "\n",
        "    plt.plot(x, y1, label='D_loss')\n",
        "    plt.plot(x, y2, label='G_loss')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.legend(loc=4)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ELqpD5tSJl22",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "            ############ TRAINING ############\n",
        "            \n",
        "def train_GAN(batch_size, epochs):\n",
        "  \n",
        "    train_hist = {}\n",
        "    train_hist['D_losses'] = []\n",
        "    train_hist['G_losses'] = []\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    x = tf.placeholder(tf.float32, shape = [None, height ,width, channels], name = 'x')\n",
        "    z = tf.placeholder(tf.float32, shape = [None, 1, 1, latent], name = 'z')\n",
        "    label = tf.placeholder(tf.float32, name = 'label', shape = [None, num_classes])\n",
        "    labeled_mask = tf.placeholder(tf.float32, name = 'labeled_mask', shape = [None])\n",
        "    dropout_rate = tf.placeholder(tf.float32, name = 'dropout_rate')\n",
        "    is_training = tf.placeholder(tf.bool, name = 'is_training')\n",
        "    \n",
        "    lr_rate = 2e-4\n",
        "    \n",
        "    model = build_GAN(x, z, dropout_rate, is_training)\n",
        "    D_real_features, D_real_logit, D_real_prob, D_fake_features, D_fake_logit, D_fake_prob, fake_data = model\n",
        "    \n",
        "    extended_label = prepare_extended_label(label)\n",
        "    \n",
        "    # Fake_data of size = batch_size*28*28*1\n",
        "    \n",
        "    loss_acc = loss_accuracy(D_real_features, D_real_logit, D_real_prob,\n",
        "                                  D_fake_features, D_fake_logit, D_fake_prob,\n",
        "                                  extended_label, labeled_mask)\n",
        "    D_L, G_L, accuracy = loss_acc\n",
        "    \n",
        "    D_optimizer, G_optimizer = optimizer(D_L, G_L, lr_rate, beta1 = 0.5)\n",
        "    \n",
        "    print ('...Training begins...')\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      \n",
        "      mnist_data = get_data()\n",
        "      no_of_batches = int (mnist_data.train.images.shape[0]/batch_size) + 1\n",
        "      \n",
        "      for epoch in range(epochs):\n",
        "        \n",
        "        train_accuracies, train_D_losses, train_G_losses = [], [], []\n",
        "        \n",
        "        for it in range(no_of_batches):\n",
        "          \n",
        "          batch = mnist_data.train.next_batch(batch_size, shuffle = False)\n",
        "          # batch[0] has shape: batch_size*28*28*1\n",
        "          \n",
        "          batch_reshaped = tf.image.resize_images(batch[0], [64, 64]).eval()\n",
        "\n",
        "          # Reshaping the whole batch into batch_size*64*64*1 for disc/gen architecture\n",
        "          batch_z = np.random.normal(0, 1, (batch_size, 1, 1, latent))\n",
        "          \n",
        "          mask = get_labeled_mask(labeled_rate, batch_size)\n",
        "                \n",
        "          train_feed_dict = {x : scale(batch_reshaped), z : batch_z,\n",
        "                                   label : batch[1], labeled_mask : mask,\n",
        "                                   dropout_rate : 0.7,\n",
        "                                   is_training : True}\n",
        "          #The label provided in dict are one hot encoded in 10 classes\n",
        "                \n",
        "          D_optimizer.run(feed_dict = train_feed_dict)\n",
        "          G_optimizer.run(feed_dict = train_feed_dict)\n",
        "                \n",
        "          train_D_loss = D_L.eval(feed_dict = train_feed_dict)\n",
        "          train_G_loss = G_L.eval(feed_dict = train_feed_dict)\n",
        "          train_accuracy = accuracy.eval(feed_dict = train_feed_dict)\n",
        "          \n",
        "          train_D_losses.append(train_D_loss)\n",
        "          train_G_losses.append(train_G_loss)\n",
        "          train_accuracies.append(train_accuracy)\n",
        "          print('Batch evaluated: ' +str(it+1))\n",
        "          \n",
        "        tr_GL = np.mean(train_G_losses)\n",
        "        tr_DL = np.mean(train_D_losses)\n",
        "        tr_acc = np.mean(train_accuracies)\n",
        "        \n",
        "        print ('After epoch: '+ str(epoch+1) + ' Generator loss: '\n",
        "                       + str(tr_GL) + ' Discriminator loss: ' + str(tr_DL) + ' Accuracy: ' + str(tr_acc))\n",
        "        \n",
        "        gen_samples = fake_data.eval(feed_dict = {z : np.random.normal(0, 1, (25, 1, 1, latent)), dropout_rate : 0.7, is_training : False})\n",
        "        # Dont train batch-norm while plotting => is_training = False\n",
        "        test_images = tf.image.resize_images(gen_samples, [64, 64]).eval()\n",
        "        show_result(test_images, (epoch + 1), show = True, save = False, path = '')\n",
        "        \n",
        "        train_hist['D_losses'].append(np.mean(train_D_losses))\n",
        "        train_hist['G_losses'].append(np.mean(train_G_losses))\n",
        "        \n",
        "      show_train_hist(train_hist, show=True, save = True, path = 'train_hist.png')\n",
        "      sess.close()\n",
        "            \n",
        "    return train_D_losses,train_G_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S9uW7Pvoi_EQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "key = train_GAN( 128 , 7)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}